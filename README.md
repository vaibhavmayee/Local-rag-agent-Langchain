# Local RAG Agent with LangChain

A **100% local**, beginner-friendly RAG (Retrieval-Augmented Generation) system using:
- **Ollama** (for LLM and embeddings)
- **LangChain â‰¥0.2.10** (for modular RAG pipeline)
- **ChromaDB** (for local vector storage)
- Supports **`.txt` and `.pdf`** files

> ğŸ”’ No internet required after setup  
> ğŸ’¡ Ideal for learning how RAG works under the hood

---
## Author : Vaibhav Vasant Mayee

## ğŸ› ï¸ Setup Instructions

### 1. Install Ollama
Ollama runs your local LLM and embedding models.

- **macOS**: [Download from ollama.com](https://ollama.com/download/Ollama-darwin.zip)  
- **Windows/Linux**: See [Ollama Install Guide](https://ollama.com/download)

After installing, **start the Ollama app** (it runs in the background).

### 2. Pull Required Models
Open **Terminal** and run:

```bash
# Embedding model (converts text â†’ vectors)
ollama pull nomic-embed-text:v1.5

# LLM for answering questions
ollama pull gemma3:4b

ğŸ’¡ These models are free, open, and run locally.
â±ï¸ First pull may take 5â€“10 minutes (downloads ~2â€“4 GB).

3. Install Python Dependencies
In your project folder, run:

bash
1
pipÂ installÂ langchain==0.2.10Â langchain-community==0.2.10Â chromadbÂ pypdfÂ ollama

âœ… This avoids dependency conflicts.
ğŸ Requires Python 3.9+.

ğŸ“ Folder Setup
Create a data/ folder and add your documents:

bash
123
mkdirÂ data#Â ThenÂ copyÂ yourÂ filesÂ intoÂ ./data/#Â Supported:Â .txtÂ andÂ .pdfÂ (text-basedÂ only)

Example:

123456
Local-rag-agent-Langchain/â”œâ”€â”€Â rag_agent.pyâ”œâ”€â”€Â data/â”‚Â Â Â â”œâ”€â”€Â notes.txtâ”‚Â Â Â â””â”€â”€Â manual.pdfâ””â”€â”€Â ...

âš ï¸ Scanned/image PDFs wonâ€™t work â€” this tool only reads text-based PDFs.

â–¶ï¸ How to Use
Step 1: Ingest Your Documents (Run Once)
bash
1
pythonÂ rag_agent.pyÂ --modeÂ embeddingÂ --data_pathÂ ./data

What this does:

Reads all .txt and .pdf files in ./data/
Splits text into chunks (800 chars with 200-char overlap)
Generates embeddings using nomic-embed-text:v1.5
Saves everything to ./chroma_db (local folder)
âœ… Success message: [SUCCESS] Ingested X chunks.

ğŸ” Run this again whenever you add new files to ./data/.

Step 2: Ask Questions
bash
1
pythonÂ rag_agent.pyÂ --modeÂ qaÂ --queryÂ "WhatÂ isÂ theÂ mainÂ ideaÂ ofÂ theÂ document?"

What this does:

Loads your saved data from ./chroma_db
Finds top 3 most relevant text chunks
Sends them + your question to gemma3:4b
Prints an answer based only on your documents
âœ… If the answer isnâ€™t in your data, it replies:
This info is not available with me right now.

ğŸ§ª Test It (Quick Demo)
Create a test file:
bash
1
echoÂ "RAGÂ standsÂ forÂ Retrieval-AugmentedÂ Generation.Â ItÂ combinesÂ retrievalÂ fromÂ aÂ knowledgeÂ baseÂ withÂ languageÂ modelÂ generation."Â >Â data/test.txt

Ingest:
bash
1
pythonÂ rag_agent.pyÂ --modeÂ embeddingÂ --data_pathÂ ./data

Ask:
bash
1
pythonÂ rag_agent.pyÂ --modeÂ qaÂ --queryÂ "WhatÂ doesÂ RAGÂ standÂ for?"

âœ… Expected output:
[ANSWER] RAG stands for Retrieval-Augmented Generation.